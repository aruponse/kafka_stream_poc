# Kafka Streams Proof of Concept (PoC) with Apache Avro

Este proyecto es una prueba de concepto completa que demuestra el uso de **Kafka Streams** con **Java 21**, **Spring Boot 3**, **Apache Kafka**, **Apache Avro** y una base de datos **H2** en memoria.

## ‚ú® Nueva Arquitectura con Apache Avro

Este proyecto ha sido refactorizado para utilizar **Apache Avro** como formato de serializaci√≥n en lugar de JSON, ofreciendo las siguientes ventajas:

- **Esquema evolutivo**: Compatibilidad hacia adelante y hacia atr√°s
- **Mejor rendimiento**: Serializaci√≥n binaria m√°s eficiente que JSON
- **Validaci√≥n de datos**: Esquemas estrictos que garantizan la integridad de los datos  
- **Generaci√≥n autom√°tica de c√≥digo**: Clases Java generadas desde esquemas Avro
- **Integraci√≥n con Schema Registry**: Gesti√≥n centralizada de schemas

## üéØ Casos de Uso Implementados

### 1. Transformaci√≥n de Contenido (Content Transformation)
- **Input**: `SimpleEvent` en el t√≥pico `input-topic`
- **Procesamiento**: Transforma el campo `payload` a may√∫sculas y a√±ade prefijo "TRANSFORMED:"
- **Output**: Evento transformado en el t√≥pico `output-topic-transformed`

### 2. Conversi√≥n de Esquema JSON (JSON Schema Conversion)
- **Input**: `LegacyEvent` en el t√≥pico `input-topic`
- **Procesamiento**: Convierte de formato legacy a `NewFormatEvent`
- **Output**: Evento convertido en el t√≥pico `output-topic-json-converted`

### 3. Enrutamiento y Divisi√≥n (Routing and Division)
- **Input**: `GenericAction` en el t√≥pico `actions-topic`
- **Procesamiento**: Enruta seg√∫n `actionType` (A o B) con transformaciones espec√≠ficas
- **Output**: Eventos procesados en `output-topic-action-a` y `output-topic-action-b`

## üèóÔ∏è Arquitectura del Proyecto

```
src/main/java/com/example/kafkastream/
‚îú‚îÄ‚îÄ KafkaStreamPocApplication.java          # Aplicaci√≥n principal
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ KafkaStreamsConfig.java            # Configuraci√≥n de Kafka Streams con Avro
‚îú‚îÄ‚îÄ controller/
‚îÇ   ‚îî‚îÄ‚îÄ EventController.java               # REST API endpoints con serializaci√≥n Avro
‚îú‚îÄ‚îÄ avro/                                  # Esquemas Avro (generados autom√°ticamente)
‚îÇ   ‚îú‚îÄ‚îÄ SimpleEvent.avsc                   # Esquema para Caso 1
‚îÇ   ‚îú‚îÄ‚îÄ LegacyEvent.avsc                   # Esquema para Caso 2 (input)
‚îÇ   ‚îú‚îÄ‚îÄ NewFormatEvent.avsc                # Esquema para Caso 2 (output)
‚îÇ   ‚îî‚îÄ‚îÄ GenericAction.avsc                 # Esquema para Caso 3
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ ProcessedEvent.java                # Entidad JPA para persistencia
‚îÇ   ‚îî‚îÄ‚îÄ OriginalEvent.java                 # Entidad JPA para tracking completo
‚îú‚îÄ‚îÄ repository/
‚îÇ   ‚îú‚îÄ‚îÄ ProcessedEventRepository.java      # Repositorio JPA para eventos procesados
‚îÇ   ‚îî‚îÄ‚îÄ OriginalEventRepository.java       # Repositorio JPA para eventos originales
‚îî‚îÄ‚îÄ service/
    ‚îú‚îÄ‚îÄ ProcessedEventService.java         # Servicio de eventos procesados
    ‚îú‚îÄ‚îÄ OriginalEventService.java          # Servicio de eventos originales
    ‚îî‚îÄ‚îÄ KafkaPersistenceService.java       # Consumidor para persistencia con Avro
```

### üìÅ Estructura de Esquemas Avro

```
src/main/avro/
‚îú‚îÄ‚îÄ SimpleEvent.avsc                       # Esquema para transformaci√≥n de contenido
‚îú‚îÄ‚îÄ LegacyEvent.avsc                       # Esquema para eventos legacy
‚îú‚îÄ‚îÄ NewFormatEvent.avsc                    # Esquema para eventos convertidos
‚îî‚îÄ‚îÄ GenericAction.avsc                     # Esquema para acciones gen√©ricas
```

### üîÑ Clases Java Generadas (target/generated-sources/avro/)

```
target/generated-sources/avro/com/example/kafkastream/avro/
‚îú‚îÄ‚îÄ SimpleEvent.java                       # Clase Avro generada
‚îú‚îÄ‚îÄ LegacyEvent.java                       # Clase Avro generada
‚îú‚îÄ‚îÄ NewFormatEvent.java                    # Clase Avro generada
‚îî‚îÄ‚îÄ GenericAction.java                     # Clase Avro generada
```

## üöÄ Requisitos Previos

1. **Java 21** instalado
2. **Maven 3.8+** instalado
3. **Apache Kafka** ejecut√°ndose en `localhost:9092`
4. **Confluent Schema Registry** ejecut√°ndose en `localhost:8081` (requerido para Avro)

### Iniciar Kafka y Schema Registry

#### Opci√≥n 1: Confluent Platform (Recomendado)
```bash
# Iniciar todos los servicios de Confluent
confluent local services start

# O iniciar servicios espec√≠ficos
confluent local services kafka start
confluent local services schema-registry start
```

#### Opci√≥n 2: Apache Kafka + Schema Registry manual
```bash
# Iniciar Zookeeper
bin/zookeeper-server-start etc/kafka/zookeeper.properties

# Iniciar Kafka Server
bin/kafka-server-start etc/kafka/server.properties

# Iniciar Schema Registry (descargar desde Confluent)
bin/schema-registry-start etc/schema-registry/schema-registry.properties
```

### Verificar que Schema Registry est√° funcionando
```bash
curl http://localhost:8081/subjects
# Deber√≠a devolver: []
```

## üì¶ Compilaci√≥n y Ejecuci√≥n

### 1. Compilar el proyecto y generar clases Avro
```bash
cd kafka_stream_poc
mvn clean compile
```

*Nota: El plugin de Maven `avro-maven-plugin` generar√° autom√°ticamente las clases Java desde los esquemas `.avsc` durante la compilaci√≥n.*

### 2. Ejecutar la aplicaci√≥n
```bash
mvn spring-boot:run
```

### 3. Verificar que est√° ejecut√°ndose
La aplicaci√≥n estar√° disponible en: `http://localhost:8082`

### 4. Verificar integraci√≥n con Schema Registry
```bash
# Despu√©s de enviar algunos eventos, verificar schemas registrados
curl http://localhost:8081/subjects

# Ver el schema de SimpleEvent
curl http://localhost:8081/subjects/input-topic-value/versions/latest
```

## üß™ Testing de los Casos de Uso

### Caso 1: Transformaci√≥n de Contenido

**Enviar SimpleEvent (JSON se convierte a Avro internamente):**
```bash
curl -X POST http://localhost:8082/api/events/simple \
  -H "Content-Type: application/json" \
  -d '{
    "id": "simple-001",
    "payload": "hello world from avro",
    "timestamp": 1640995200000
  }'
```

### Caso 2: Conversi√≥n de Esquema (Legacy a New Format)

**Enviar LegacyEvent (JSON se convierte a Avro internamente):**
```bash
curl -X POST http://localhost:8082/api/events/legacy \
  -H "Content-Type: application/json" \
  -d '{
    "old_field_name": "legacy_field",
    "value": "legacy data processed via avro"
  }'
```

### Caso 3: Enrutamiento y Divisi√≥n

**Enviar GenericAction Tipo A (JSON se convierte a Avro internamente):**
```bash
curl -X POST http://localhost:8082/api/events/action \
  -H "Content-Type: application/json" \
  -d '{
    "actionType": "A",
    "details": "Process action type A via avro serialization"
  }'
```

**Enviar GenericAction Tipo B (JSON se convierte a Avro internamente):**
```bash
curl -X POST http://localhost:8082/api/events/action \
  -H "Content-Type: application/json" \
  -d '{
    "actionType": "B",
    "details": "Process action type B via avro serialization"
  }'
```

### Consultar Eventos Procesados

**Ver todos los eventos procesados:**
```bash
curl -X GET http://localhost:8082/api/events/processed
```

**Ver estad√≠sticas:**
```bash
curl -X GET http://localhost:8082/api/events/processed/stats
```

**Filtrar por tipo de evento:**
```bash
curl -X GET "http://localhost:8082/api/events/processed?eventType=SIMPLE_EVENT_TRANSFORMED"
```

## üíæ Base de Datos H2

### Acceder a la Consola H2
- URL: `http://localhost:8082/h2-console`
- JDBC URL: `jdbc:h2:mem:testdb`
- Username: `sa`
- Password: (vac√≠o)

### Consultar la Tabla de Eventos Procesados
```sql
SELECT * FROM processed_events ORDER BY processed_at DESC;
```

## üìã Endpoints de la API REST

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| POST | `/api/events/simple` | Publicar SimpleEvent (JSON‚ÜíAvro) |
| POST | `/api/events/legacy` | Publicar LegacyEvent (JSON‚ÜíAvro) |
| POST | `/api/events/action` | Publicar GenericAction (JSON‚ÜíAvro) |
| GET | `/api/events/processed` | Obtener todos los eventos procesados |
| GET | `/api/events/processed/stats` | Obtener estad√≠sticas de procesamiento |
| GET | `/api/events/original` | Obtener todos los eventos originales |
| DELETE | `/api/events/processed` | Eliminar todos los eventos procesados |
| DELETE | `/api/events/original` | Eliminar todos los eventos originales |

## üîß Detalles T√©cnicos de Avro

### Esquemas Avro Definidos

#### SimpleEvent.avsc
```json
{
  "type": "record",
  "name": "SimpleEvent",
  "namespace": "com.example.kafkastream.avro",
  "doc": "Simple event for Kafka Streams Use Case 1: Content Transformation",
  "fields": [
    {"name": "id", "type": "string", "doc": "Unique identifier for the event"},
    {"name": "payload", "type": "string", "doc": "The main content/payload of the event"},
    {"name": "timestamp", "type": "long", "doc": "Timestamp when the event was created (epoch milliseconds)"}
  ]
}
```

#### LegacyEvent.avsc  
```json
{
  "type": "record",
  "name": "LegacyEvent",
  "namespace": "com.example.kafkastream.avro",
  "doc": "Legacy event format for Kafka Streams Use Case 2: JSON Schema Conversion",
  "fields": [
    {"name": "old_field_name", "type": "string", "doc": "Legacy field name (to be converted)"},
    {"name": "value", "type": "string", "doc": "Value to be converted to new format"}
  ]
}
```

### Beneficios de Avro vs JSON

| Aspecto | JSON | Avro |
|---------|------|------|
| **Tama√±o** | Texto plano, mayor tama√±o | Binario compacto, menor tama√±o |
| **Rendimiento** | Parsing lento | Serializaci√≥n/deserializaci√≥n r√°pida |
| **Evoluci√≥n** | Sin garant√≠as | Esquema evolutivo con compatibilidad |
| **Validaci√≥n** | Manual/externa | Autom√°tica por esquema |
| **Tipado** | Din√°mico | Est√°tico y fuerte |
| **Interoperabilidad** | Dependiente de librer√≠a | Est√°ndar cross-platform |

### Configuraci√≥n de Avro Serdes

La aplicaci√≥n est√° configurada para usar Avro con Schema Registry:

```yaml
spring:
  kafka:
    producer:
      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
    consumer:  
      value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
    streams:
      properties:
        default.value.serde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
        schema.registry.url: http://localhost:8081
```

## üîß Configuraci√≥n

### T√≥picos de Kafka (configurables en `application.yml`)
- `input-topic`: Entrada para casos 1 y 2
- `actions-topic`: Entrada para caso 3
- `output-topic-transformed`: Salida para caso 1
- `output-topic-json-converted`: Salida para caso 2
- `output-topic-action-a`: Salida para acciones tipo A
- `output-topic-action-b`: Salida para acciones tipo B

### Variables de Configuraci√≥n Principales
```yaml
app:
  kafka:
    topics:
      input-topic: input-topic
      actions-topic: actions-topic
      output-topic-transformed: output-topic-transformed
      output-topic-json-converted: output-topic-json-converted
      output-topic-action-a: output-topic-action-a
      output-topic-action-b: output-topic-action-b
```

## üêõ Troubleshooting

### Problema: Kafka no est√° ejecut√°ndose
**Error**: `Connection to node -1 could not be established`
**Soluci√≥n**: Verificar que Kafka est√© ejecut√°ndose en `localhost:9092`

### Problema: T√≥picos no se crean autom√°ticamente
**Soluci√≥n**: Crear t√≥picos manualmente:
```bash
kafka-topics --create --topic input-topic --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
kafka-topics --create --topic actions-topic --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
kafka-topics --create --topic output-topic-transformed --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
kafka-topics --create --topic output-topic-json-converted --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
kafka-topics --create --topic output-topic-action-a --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
kafka-topics --create --topic output-topic-action-b --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
```

## üìä Monitoreo y Logs

Los logs de la aplicaci√≥n mostrar√°n el procesamiento de eventos en tiempo real:
- Procesamiento de Kafka Streams
- Persistencia de eventos en H2
- Actividad de la API REST

### Nivel de Log para Desarrollo
```yaml
logging:
  level:
    com.example.kafkastream: DEBUG
    org.springframework.kafka: DEBUG
```

## üéâ Flujo Completo de Trabajo

1. **Publicar** evento via REST API
2. **Procesar** evento con Kafka Streams (transformaci√≥n/conversi√≥n/enrutamiento)
3. **Persistir** resultado en base de datos H2 via @KafkaListener
4. **Consultar** eventos procesados via REST API

Este PoC demuestra un pipeline completo de procesamiento de eventos con Kafka Streams, desde la ingesta hasta la persistencia, pasando por transformaciones complejas de datos.