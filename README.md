# Kafka Streams Proof of Concept (PoC) with JSON Serialization

Este proyecto es una prueba de concepto completa que demuestra el uso de **Kafka Streams** con **Java 21**, **Spring Boot 3**, **Apache Kafka** y una base de datos **H2** en memoria, utilizando **JSON** como formato de serializaciÃ³n.

## âœ¨ Arquitectura con JSON Serialization

Este proyecto utiliza **JSON** como formato de serializaciÃ³n, ofreciendo las siguientes ventajas:

- **Facilidad de desarrollo**: Formato legible y fÃ¡cil de debuggear
- **Flexibilidad**: Ideal para prototipos y desarrollo rÃ¡pido
- **Compatibilidad universal**: Compatible con cualquier cliente HTTP
- **Menor complejidad**: Sin necesidad de Schema Registry
- **Testing simplificado**: FÃ¡cil creaciÃ³n de datos de prueba

## ğŸ¯ Casos de Uso Implementados

### 1. TransformaciÃ³n de Contenido (Content Transformation)
- **Input**: `SimpleEvent` en el tÃ³pico `input-topic`
- **Procesamiento**: Transforma el campo `payload` aÃ±adiendo prefijo "TRANSFORMED:"
- **Output**: Evento transformado en el tÃ³pico `output-topic-transformed`

### 2. ConversiÃ³n de Esquema JSON (JSON Schema Conversion)
- **Input**: `LegacyEvent` en el tÃ³pico `legacy-events-topic`
- **Procesamiento**: Convierte de formato legacy a `NewFormatEvent`
- **Output**: Evento convertido en el tÃ³pico `output-topic-json-converted`

### 3. Enrutamiento y DivisiÃ³n (Routing and Division)
- **Input**: `GenericAction` en el tÃ³pico `actions-topic`
- **Procesamiento**: Enruta segÃºn `actionType` (A o B) con transformaciones especÃ­ficas
- **Output**: Eventos procesados en `output-topic-action-a` y `output-topic-action-b`

### 4. Procesamiento de Mensajes Entrantes (Inbound Message Processing) ğŸ†•
- **Input**: `InboundMessageEvent` en el tÃ³pico `inbound-message-topic`
- **Procesamiento**: Transforma un mensaje entrante complejo en dos eventos diferentes
- **Output**: 
  - `CreateChatEvent` en el tÃ³pico `create-chat-topic`
  - `CreateMessageEvent` en el tÃ³pico `create-message-topic`

## ğŸ—ï¸ Arquitectura del Proyecto

```
src/main/java/com/example/kafkastream/
â”œâ”€â”€ KafkaStreamPocApplication.java          # AplicaciÃ³n principal
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ KafkaStreamsConfig.java            # ConfiguraciÃ³n de Kafka Streams con JSON
â”‚   â””â”€â”€ KafkaConsumerConfig.java           # ConfiguraciÃ³n de consumidores Kafka
â”œâ”€â”€ controller/
â”‚   â””â”€â”€ EventController.java               # REST API endpoints
â”œâ”€â”€ dto/                                   # Data Transfer Objects
â”‚   â”œâ”€â”€ SimpleEvent.java                   # DTO para Caso 1
â”‚   â”œâ”€â”€ LegacyEvent.java                   # DTO para Caso 2 (input)
â”‚   â”œâ”€â”€ NewFormatEvent.java                # DTO para Caso 2 (output)
â”‚   â”œâ”€â”€ GenericAction.java                 # DTO para Caso 3
â”‚   â”œâ”€â”€ InboundMessageEvent.java           # DTO para Caso 4 (input) ğŸ†•
â”‚   â”œâ”€â”€ CreateChatEvent.java               # DTO para Caso 4 (output) ğŸ†•
â”‚   â””â”€â”€ CreateMessageEvent.java            # DTO para Caso 4 (output) ğŸ†•
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ ProcessedEvent.java                # Entidad JPA para persistencia
â”‚   â””â”€â”€ OriginalEvent.java                 # Entidad JPA para tracking completo
â”œâ”€â”€ repository/
â”‚   â”œâ”€â”€ ProcessedEventRepository.java      # Repositorio JPA para eventos procesados
â”‚   â””â”€â”€ OriginalEventRepository.java       # Repositorio JPA para eventos originales
â”œâ”€â”€ serde/
â”‚   â””â”€â”€ JsonSerde.java                     # Serializador/Deserializador JSON personalizado
â””â”€â”€ service/
    â”œâ”€â”€ ProcessedEventService.java         # Servicio de eventos procesados
    â”œâ”€â”€ OriginalEventService.java          # Servicio de eventos originales
    â””â”€â”€ KafkaPersistenceService.java       # Consumidor para persistencia
```

### ğŸ“ Ejemplos de Datos de Prueba

```
examples/
â”œâ”€â”€ simple-event.json                      # Ejemplo para Caso 1
â”œâ”€â”€ legacy-event.json                      # Ejemplo para Caso 2
â”œâ”€â”€ action-a.json                          # Ejemplo para Caso 3A
â”œâ”€â”€ action-b.json                          # Ejemplo para Caso 3B
â””â”€â”€ inbound-message-event.json             # Ejemplo para Caso 4 ğŸ†•
```

## ğŸš€ Requisitos Previos

1. **Java 21** instalado
2. **Maven 3.8+** instalado
3. **Apache Kafka** ejecutÃ¡ndose en `localhost:9092`
4. **Docker** (opcional, para usar docker-compose)

### Iniciar Kafka

#### OpciÃ³n 1: Docker Compose (Recomendado)
```bash
# Usar el docker-compose.yml incluido en el proyecto
docker-compose up -d
```

#### OpciÃ³n 2: Kafka Local
```bash
# Iniciar Zookeeper
bin/zookeeper-server-start config/zookeeper.properties

# Iniciar Kafka Server
bin/kafka-server-start config/server.properties
```

### Crear TÃ³picos
```bash
# Usar el script incluido
./create-topics.sh
```

## ğŸ“¦ CompilaciÃ³n y EjecuciÃ³n

### 1. Compilar el proyecto
```bash
cd kafka_stream_poc
mvn clean compile
```

### 2. Ejecutar la aplicaciÃ³n
```bash
mvn spring-boot:run
```

### 3. Verificar que estÃ¡ ejecutÃ¡ndose
La aplicaciÃ³n estarÃ¡ disponible en: `http://localhost:8082`

### 4. Ejecutar pruebas automatizadas
```bash
# Usar el script de testing incluido
./test-api.sh
```

## ğŸ§ª Testing de los Casos de Uso

### Caso 1: TransformaciÃ³n de Contenido

**Enviar SimpleEvent:**
```bash
curl -X POST http://localhost:8082/api/events/simple \
  -H "Content-Type: application/json" \
  -d '{
    "id": "simple-001",
    "payload": "hello world",
    "timestamp": 1640995200000
  }'
```

### Caso 2: ConversiÃ³n de Esquema (Legacy a New Format)

**Enviar LegacyEvent:**
```bash
curl -X POST http://localhost:8082/api/events/legacy \
  -H "Content-Type: application/json" \
  -d '{
    "old_field_name": "legacy_field",
    "value": "legacy data to be converted"
  }'
```

### Caso 3: Enrutamiento y DivisiÃ³n

**Enviar GenericAction Tipo A:**
```bash
curl -X POST http://localhost:8082/api/events/action \
  -H "Content-Type: application/json" \
  -d '{
    "actionType": "A",
    "details": "Process action type A"
  }'
```

**Enviar GenericAction Tipo B:**
```bash
curl -X POST http://localhost:8082/api/events/action \
  -H "Content-Type: application/json" \
  -d '{
    "actionType": "B",
    "details": "Process action type B"
  }'
```

### Caso 4: Procesamiento de Mensajes Entrantes ğŸ†•

**Enviar InboundMessageEvent:**
```bash
curl -X POST http://localhost:8082/api/events/inbound-message \
  -H "Content-Type: application/json" \
  -d '{
    "app": "TestApp",
    "timestamp": 1747854609182,
    "version": 2,
    "type": "message",
    "payload": {
      "id": "wamid.HBgMNTkzxhgfjg3Nxc5NzU3FZCqEazxcE1ODg0QUIzQTg4NjUa4NUR1BQzYB",
      "source": "123456789011",
      "type": "text",
      "payload": { "text": "Hola desde WhatsApp" },
      "sender": { 
        "phone": "123456789011",
        "name": "Online UserName",
        "country_code": "1",
        "dial_code": "23456789011"
      }
    }
  }'
```

### Consultar Eventos Procesados

**Ver todos los eventos procesados:**
```bash
curl -X GET http://localhost:8082/api/events/processed
```

**Ver estadÃ­sticas:**
```bash
curl -X GET http://localhost:8082/api/events/processed/stats
```

**Filtrar por tipo de evento:**
```bash
curl -X GET "http://localhost:8082/api/events/processed?eventType=SIMPLE_EVENT_TRANSFORMED"
```

## ğŸ’¾ Base de Datos H2

### Acceder a la Consola H2
- URL: `http://localhost:8082/h2-console`
- JDBC URL: `jdbc:h2:mem:testdb`
- Username: `sa`
- Password: (vacÃ­o)

### Consultar la Tabla de Eventos Procesados
```sql
SELECT * FROM processed_events ORDER BY processed_at DESC;
```

## ğŸ“‹ Endpoints de la API REST

| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|----------|-------------|
| POST | `/api/events/simple` | Publicar SimpleEvent |
| POST | `/api/events/legacy` | Publicar LegacyEvent |
| POST | `/api/events/action` | Publicar GenericAction |
| POST | `/api/events/inbound-message` | Publicar InboundMessageEvent ğŸ†• |
| GET | `/api/events/processed` | Obtener todos los eventos procesados |
| GET | `/api/events/processed/stats` | Obtener estadÃ­sticas de procesamiento |
| GET | `/api/events/original` | Obtener todos los eventos originales |
| DELETE | `/api/events/processed` | Eliminar todos los eventos procesados |
| DELETE | `/api/events/original` | Eliminar todos los eventos originales |

## ğŸ”§ Detalles TÃ©cnicos de JSON

### Estructura de DTOs Principales

#### InboundMessageEvent (Caso 4) ğŸ†•
Representa un mensaje entrante complejo de WhatsApp con estructura anidada:
```json
{
  "app": "TestApp",
  "timestamp": 1747854609182,
  "version": 2,
  "type": "message",
  "payload": {
    "id": "wamid.HBgMNTkz...",
    "source": "123456789011",
    "type": "text",
    "payload": { "text": "Mensaje del usuario" },
    "sender": { 
      "phone": "123456789011",
      "name": "Online UserName",
      "country_code": "1",
      "dial_code": "23456789011"
    }
  }
}
```

#### CreateChatEvent (Salida del Caso 4)
```json
{
  "chat_id": "chat_123456789011_1747854609182",
  "user_name": "Online UserName",
  "user_phone": "123456789011",
  "country_code": "1",
  "dial_code": "23456789011",
  "created_at": 1747854609182
}
```

#### CreateMessageEvent (Salida del Caso 4)
```json
{
  "message_id": "wamid.HBgMNTkz...",
  "sender_phone": "123456789011",
  "chat_id": "chat_123456789011_1747854609182",
  "message_type": "text",
  "content": "Mensaje del usuario",
  "timestamp": 1747854609182
}
```

### ConfiguraciÃ³n de JSON Serdes

La aplicaciÃ³n estÃ¡ configurada para usar JSON nativo de Spring Kafka:

```yaml
spring:
  kafka:
    producer:
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
    consumer:  
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        "[spring.json.trusted.packages]": "com.example.kafkastream.dto"
    streams:
      properties:
        "[default.value.serde]": org.apache.kafka.common.serialization.Serdes$StringSerde
```

## ğŸ”§ ConfiguraciÃ³n

### TÃ³picos de Kafka (configurables en `application.yml`)
- `input-topic`: Entrada para caso 1
- `legacy-events-topic`: Entrada para caso 2
- `actions-topic`: Entrada para caso 3
- `inbound-message-topic`: Entrada para caso 4 ğŸ†•
- `output-topic-transformed`: Salida para caso 1
- `output-topic-json-converted`: Salida para caso 2
- `output-topic-action-a`: Salida para acciones tipo A
- `output-topic-action-b`: Salida para acciones tipo B
- `create-chat-topic`: Salida para crear chat (caso 4) ğŸ†•
- `create-message-topic`: Salida para crear mensaje (caso 4) ğŸ†•

### Variables de ConfiguraciÃ³n Principales
```yaml
app:
  kafka:
    topics:
      input-topic: input-topic
      legacy-events-topic: legacy-events-topic
      actions-topic: actions-topic
      inbound-message-topic: inbound-message-topic
      output-topic-transformed: output-topic-transformed
      output-topic-json-converted: output-topic-json-converted
      output-topic-action-a: output-topic-action-a
      output-topic-action-b: output-topic-action-b
      create-chat-topic: create-chat-topic
      create-message-topic: create-message-topic
```

## ğŸ› Troubleshooting

### Problema: Kafka no estÃ¡ ejecutÃ¡ndose
**Error**: `Connection to node -1 could not be established`
**SoluciÃ³n**: Verificar que Kafka estÃ© ejecutÃ¡ndose en `localhost:9092`

### Problema: TÃ³picos no se crean automÃ¡ticamente
**SoluciÃ³n**: Crear tÃ³picos manualmente:
```bash
# O usar el script automatizado incluido
./create-topics.sh

# TÃ³picos creados automÃ¡ticamente:
# - input-topic
# - legacy-events-topic  
# - actions-topic
# - inbound-message-topic
# - output-topic-transformed
# - output-topic-json-converted
# - output-topic-action-a
# - output-topic-action-b
# - create-chat-topic
# - create-message-topic
```

## ğŸ“Š Monitoreo y Logs

Los logs de la aplicaciÃ³n mostrarÃ¡n el procesamiento de eventos en tiempo real:
- Procesamiento de Kafka Streams
- Persistencia de eventos en H2
- Actividad de la API REST

### Nivel de Log para Desarrollo
```yaml
logging:
  level:
    com.example.kafkastream: DEBUG
    org.springframework.kafka: DEBUG
```

## ğŸ‰ Flujo Completo de Trabajo

1. **Publicar** evento via REST API
2. **Procesar** evento con Kafka Streams (transformaciÃ³n/conversiÃ³n/enrutamiento/divisiÃ³n)
3. **Persistir** resultado en base de datos H2 via @KafkaListener
4. **Consultar** eventos procesados via REST API

Este PoC demuestra un pipeline completo de procesamiento de eventos con Kafka Streams, desde la ingesta hasta la persistencia, pasando por transformaciones complejas de datos que incluyen:

- **TransformaciÃ³n simple** de contenido
- **ConversiÃ³n de esquemas** legacy a nuevos formatos
- **Enrutamiento condicional** basado en tipos de acciÃ³n
- **DivisiÃ³n de eventos complejos** en mÃºltiples eventos de salida ğŸ†•